{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Policy Gradient Descent using Tensorflow\n",
    "Used to solve a multi-armed bandit problem.\n",
    "\n",
    "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The bandits\n",
    "Four bandits that has a probability to generate a positive reward. Lower score means higher probability to generate a positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of bandits rewards\n",
    "bandits = [0.2, 0, 0.2, -5]\n",
    "num_bandits = len(bandits)\n",
    "\n",
    "def pullArm(bandit):\n",
    "    result = np.random.randn(1) # normal distribution\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agent\n",
    "The agent has a weight for each bandit. The value is an estimate of the value of return from choosing the bandit.\n",
    "We use a policy gradient method to update the agent by moving the for the selected action toward the recieved reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Feed forward network\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# Training with backprop\n",
    "lr = 0.001\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent\n",
    "The agent takes action in our enviorment and receives rewards. Using rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for the bandits:  [-1.  0.  0.  0.]\n",
      "Reward for the bandits:  [ -2.  -2.  -2.  45.]\n",
      "Reward for the bandits:  [ -3.  -2.  -3.  93.]\n",
      "Reward for the bandits:  [  -5.   -3.   -3.  140.]\n",
      "Reward for the bandits:  [  -5.   -5.   -3.  182.]\n",
      "Reward for the bandits:  [  -5.   -6.   -3.  229.]\n",
      "Reward for the bandits:  [  -7.   -6.   -2.  274.]\n",
      "Reward for the bandits:  [  -8.   -6.   -4.  321.]\n",
      "Reward for the bandits:  [  -8.   -5.   -4.  368.]\n",
      "Reward for the bandits:  [  -9.   -6.   -5.  415.]\n",
      "Reward for the bandits:  [ -13.   -5.   -5.  460.]\n",
      "Reward for the bandits:  [ -12.   -4.   -6.  501.]\n",
      "Reward for the bandits:  [ -12.   -4.   -6.  551.]\n",
      "Reward for the bandits:  [ -14.   -4.   -4.  595.]\n",
      "Reward for the bandits:  [ -13.   -5.   -6.  641.]\n",
      "Reward for the bandits:  [ -12.   -5.   -6.  688.]\n",
      "Reward for the bandits:  [ -12.   -5.   -6.  734.]\n",
      "Reward for the bandits:  [ -12.   -4.   -6.  779.]\n",
      "Reward for the bandits:  [ -14.   -4.   -5.  824.]\n",
      "Reward for the bandits:  [ -12.   -4.   -6.  871.]\n",
      "Agent learning that the best bandit is:  4\n",
      "Learned weights:  [ 0.98693132  0.99499905  0.99399501  1.68419647]\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000\n",
    "total_rewards = np.zeros(num_bandits)\n",
    "e = 0.1 # exploration probability (taking random action)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    i = 0\n",
    "    \n",
    "    while i < total_episodes:\n",
    "        \n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = session.run(chosen_action)\n",
    "        \n",
    "        reward = pullArm(bandits[action]) # Get reward from bandit\n",
    "        \n",
    "        # Update network\n",
    "        _, resp, ww = session.run([update, responsible_weight, weights], feed_dict={reward_holder: [reward], action_holder: [action]})\n",
    "        \n",
    "        # Update our running tally of scores\n",
    "        total_rewards[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print('Reward for the bandits: ', total_rewards)\n",
    "        i += 1\n",
    "print('Agent learning that the best bandit is: ', np.argmax(ww) + 1)\n",
    "print('Learned weights: ', ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
